{
  "timestamp": "2025-07-06T13:10:25.426Z",
  "totalItems": 40,
  "items": [
    {
      "id": "arxiv-2507.02851v1-1751807424051",
      "title": "MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs",
      "description": "Recent advancements in the reasoning capabilities of large language models (LLMs) show that employing group relative policy optimization (GRPO) algorithm for reinforcement learning (RL) training allows the models to use more thinking/reasoning tokens for generating better responses. However, LLMs can generate only a finite amount of tokens while maintaining attention to the previously generated tokens. This limit, also known as the context size of an LLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens. To think beyond the limit of context size, an LLM must employ a modular thinking strategy to reason over multiple rounds. In this work, we propose $\\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL training method for generating thinking tokens in multiple rounds, effectively allowing the model to think with additional context size. We trained the open-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient fine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our experiments show 3.8\\% and 3.3\\% improvements over vanilla GRPO based training in the respective benchmarks. Furthermore, this improvement was achieved with only 15\\% of samples, thus demonstrating sample efficiency of MOTIF. Our code and models are available at https://github.com/purbeshmitra/MOTIF and https://huggingface.co/purbeshmitra/MOTIF, respectively.",
      "url": "https://arxiv.org/abs/2507.02851v1",
      "source": "ArXiv Domain",
      "timestamp": "2025-07-03T17:55:43.000Z",
      "metadata": {
        "arxivId": "2507.02851v1",
        "authors": "Purbesh Mitra, Sennur Ulukus",
        "categories": "cs.CL, cs.AI, cs.IT, cs.LG, cs.SY, eess.SY, math.IT",
        "published": "2025-07-03T17:55:43Z",
        "pdfUrl": "https://arxiv.org/pdf/2507.02851v1.pdf",
        "abstractUrl": "https://arxiv.org/abs/2507.02851v1",
        "rank": 1,
        "domain": "LLM"
      }
    },
    {
      "id": "arxiv-2507.02850v1-1751807424051",
      "title": "LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection to All Users",
      "description": "We describe a vulnerability in language models (LMs) trained with user feedback, whereby a single user can persistently alter LM knowledge and behavior given only the ability to provide prompts and upvote / downvote feedback on LM outputs. To implement the attack, the attacker prompts the LM to stochastically output either a \"poisoned\" or benign response, then upvotes the poisoned response or downvotes the benign one. When feedback signals are used in a subsequent preference tuning behavior, LMs exhibit increased probability of producing poisoned responses even in contexts without malicious prompts. We show that this attack can be used to (1) insert factual knowledge the model did not previously possess, (2) modify code generation patterns in ways that introduce exploitable security flaws, and (3) inject fake financial news. Our finding both identifies a new qualitative feature of language model preference tuning (showing that it even highly restricted forms of preference data can be used to exert fine-grained control over behavior), and a new attack mechanism for LMs trained with user feedback (extending work on pretraining-time data poisoning and deployment-time prompt injection).",
      "url": "https://arxiv.org/abs/2507.02850v1",
      "source": "ArXiv Domain",
      "timestamp": "2025-07-03T17:55:40.000Z",
      "metadata": {
        "arxivId": "2507.02850v1",
        "authors": "Almog Hilel, Idan Shenfeld, Leshem Choshen, Jacob Andreas",
        "categories": "cs.CL, cs.CR, cs.LG",
        "published": "2025-07-03T17:55:40Z",
        "pdfUrl": "https://arxiv.org/pdf/2507.02850v1.pdf",
        "abstractUrl": "https://arxiv.org/abs/2507.02850v1",
        "rank": 2,
        "domain": "LLM"
      }
    },
    {
      "id": "arxiv-2502.11268v2-1751807424051",
      "title": "Improved Unbiased Watermark for Large Language Models",
      "description": "As artificial intelligence surpasses human capabilities in text generation, the necessity to authenticate the origins of AI-generated content has become paramount. Unbiased watermarks offer a powerful solution by embedding statistical signals into language model-generated text without distorting the quality. In this paper, we introduce MCmark, a family of unbiased, Multi-Channel-based watermarks. MCmark works by partitioning the model's vocabulary into segments and promoting token probabilities within a selected segment based on a watermark key. We demonstrate that MCmark not only preserves the original distribution of the language model but also offers significant improvements in detectability and robustness over existing unbiased watermarks. Our experiments with widely-used language models demonstrate an improvement in detectability of over 10% using MCmark, compared to existing state-of-the-art unbiased watermarks. This advancement underscores MCmark's potential in enhancing the practical application of watermarking in AI-generated texts.",
      "url": "https://arxiv.org/abs/2502.11268v2",
      "source": "ArXiv Domain",
      "timestamp": "2025-02-16T21:02:36.000Z",
      "metadata": {
        "arxivId": "2502.11268v2",
        "authors": "Ruibo Chen, Yihan Wu, Junfeng Guo, Heng Huang",
        "categories": "cs.CL",
        "published": "2025-02-16T21:02:36Z",
        "pdfUrl": "https://arxiv.org/pdf/2502.11268v2.pdf",
        "abstractUrl": "https://arxiv.org/abs/2502.11268v2",
        "rank": 3,
        "domain": "LLM"
      }
    },
    {
      "id": "arxiv-2507.02822v1-1751807424051",
      "title": "SynapseRoute: An Auto-Route Switching Framework on Dual-State Large Language Model",
      "description": "With the widespread adoption of large language models (LLMs) in practical applications, selecting an appropriate model requires balancing not only performance but also operational cost. The emergence of reasoning-capable models has further widened the cost gap between \"thinking\" (high reasoning) and \"non-thinking\" (fast, low-cost) modes. In this work, we reveal that approximately 58% of medical questions can be accurately answered by the non-thinking mode alone, without requiring the high-cost reasoning process. This highlights a clear dichotomy in problem complexity and suggests that dynamically routing queries to the appropriate mode based on complexity could optimize accuracy, cost-efficiency, and overall user experience. Based on this, we further propose SynapseRoute, a machine learning-based dynamic routing framework that intelligently assigns input queries to either thinking or non-thinking modes. Experimental results on several medical datasets demonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs. 0.8272) compared to the thinking mode alone but also reduces inference time by 36.8% and token consumption by 39.66%. Importantly, qualitative analysis indicates that over-reasoning on simpler queries can lead to unnecessary delays and even decreased accuracy, a pitfall avoided by our adaptive routing. Finally, this work further introduces the Accuracy-Inference-Token (AIT) index to comprehensively evaluate the trade-offs among accuracy, latency, and token cost.",
      "url": "https://arxiv.org/abs/2507.02822v1",
      "source": "ArXiv Domain",
      "timestamp": "2025-07-03T17:33:58.000Z",
      "metadata": {
        "arxivId": "2507.02822v1",
        "authors": "Wencheng Zhang, Shiqin Qiao, Lingjie Luo, Yinfeng Li, Chuanyang Zheng, Qian Xu, Meng Li, Yong Gui, Yijun He, Jianing Qiu, Jindong Hong, Jiankai Sun",
        "categories": "cs.CL, cs.AI, cs.LG",
        "published": "2025-07-03T17:33:58Z",
        "pdfUrl": "https://arxiv.org/pdf/2507.02822v1.pdf",
        "abstractUrl": "https://arxiv.org/abs/2507.02822v1",
        "rank": 4,
        "domain": "LLM"
      }
    },
    {
      "id": "arxiv-2506.22049v2-1751807424051",
      "title": "GPAS: Accelerating Convergence of LLM Pretraining via Gradient-Preserving Activation Scaling",
      "description": "Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series, predominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While being stable during pretraining and scalable to large model sizes, Pre-LN suffers from an exponential growth in activation variance across layers, causing the shortcut to dominate over sub-layer outputs in the residual connection and limiting the learning capacity of deeper layers. To mitigate this issue, we propose Gradient-Preserving Activation Scaling (GPAS), a simple technique that can be used in combination with existing approaches. GPAS works by scaling down the intermediate activations while keeping their gradients unchanged. This leaves information in the activations intact, and avoids the gradient vanishing problem associated with gradient downscaling. Extensive experiments across various model sizes from 71M to 1B show that GPAS achieves consistent performance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows promise in improving alternative architectures such as Sandwich-LN and DeepNorm, demonstrating its versatility and potential for improving training dynamics in a wide range of settings. Our code is available at https://github.com/dandingsky/GPAS.",
      "url": "https://arxiv.org/abs/2506.22049v2",
      "source": "ArXiv Domain",
      "timestamp": "2025-06-27T09:45:15.000Z",
      "metadata": {
        "arxivId": "2506.22049v2",
        "authors": "Tianhao Chen, Xin Xu, Zijing Liu, Pengxiang Li, Xinyuan Song, Ajay Kumar Jaiswal, Fan Zhang, Jishan Hu, Yang Wang, Hao Chen, Shizhe Diao, Shiwei Liu, Yu Li, Lu Yin, Can Yang",
        "categories": "cs.LG, cs.CL",
        "published": "2025-06-27T09:45:15Z",
        "pdfUrl": "https://arxiv.org/pdf/2506.22049v2.pdf",
        "abstractUrl": "https://arxiv.org/abs/2506.22049v2",
        "rank": 5,
        "domain": "LLM"
      }
    },
    {
      "id": "arxiv-2507.02778v1-1751807424051",
      "title": "Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs",
      "description": "Although large language models (LLMs) have become transformative, they still make mistakes and can explore unproductive reasoning paths. Self-correction is an important capability for a trustworthy LLM, particularly an autoregressive LLM. While LLMs can identify error in user input, they exhibit a systematic 'Self-Correction Blind Spot' - failing to correct identical error in their own outputs. To systematically study this phenomenon, we introduce Self-Correction Bench, a systematic framework to measure this phenomenon through controlled error injection at three complexity levels. Testing 14 models, we find an average 64.5% blind spot rate. We find multiple evidences that this limitation relates to training data composition: human training demonstrations predominantly show error-free responses rather than error-correction sequences, unlike RL-trained models that learn error correction through outcome feedback. Remarkably, simply appending \"Wait\" reduces blind spots by 89.3%, suggesting that the capability exists but requires activation. Our work highlights a critical limitation in current LLMs and offers potential avenues for improving their reliability and trustworthiness.",
      "url": "https://arxiv.org/abs/2507.02778v1",
      "source": "ArXiv Domain",
      "timestamp": "2025-07-03T16:41:30.000Z",
      "metadata": {
        "arxivId": "2507.02778v1",
        "authors": "Ken Tsui",
        "categories": "cs.CL, cs.AI, cs.LG",
        "published": "2025-07-03T16:41:30Z",
        "pdfUrl": "https://arxiv.org/pdf/2507.02778v1.pdf",
        "abstractUrl": "https://arxiv.org/abs/2507.02778v1",
        "rank": 6,
        "domain": "LLM"
      }
    },
    {
      "id": "arxiv-2412.05693v3-1751807424051",
      "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache Compression",
      "description": "Several works have developed eviction policies to remove key-value (KV) pairs from the KV cache for more efficient inference. The focus has been on compressing the KV cache after the input prompt has been processed for faster token generation. In settings with limited GPU memory, and when the input context is longer than the generation length, we show that by also compressing the KV cache during the input processing phase, larger batch sizes can be used resulting in significantly higher throughput while still maintaining the original model's accuracy.",
      "url": "https://arxiv.org/abs/2412.05693v3",
      "source": "ArXiv Domain",
      "timestamp": "2024-12-07T16:41:54.000Z",
      "metadata": {
        "arxivId": "2412.05693v3",
        "authors": "Michael R. Metel, Boxing Chen, Mehdi Rezagholizadeh",
        "categories": "cs.CL",
        "published": "2024-12-07T16:41:54Z",
        "pdfUrl": "https://arxiv.org/pdf/2412.05693v3.pdf",
        "abstractUrl": "https://arxiv.org/abs/2412.05693v3",
        "rank": 7,
        "domain": "LLM"
      }
    },
    {
      "id": "arxiv-2507.02737v1-1751807424051",
      "title": "Early Signs of Steganographic Capabilities in Frontier LLMs",
      "description": "Monitoring Large Language Model (LLM) outputs is crucial for mitigating risks from misuse and misalignment. However, LLMs could evade monitoring through steganography: Encoding hidden information within seemingly benign generations. In this paper, we evaluate the steganography capabilities in frontier LLMs to better understand the risk they pose. We focus on two types of steganography: passing encoded messages and performing encoded reasoning. We find that current models are unable to encode short messages in their outputs without a monitor noticing under standard affordances. They can succeed, however, if given additional affordances such as using an unmonitored scratchpad and coordinating on what encoding scheme to use. We additionally find early signs that models can perform basic encoded reasoning in a simple state-tracking problem. This includes some ability to reason with their own and pre-defined schemes, including encoding schemes such as Hexadecimal. Despite this, they can rarely hide reasoning subtly within a cover task to fool a monitor. Overall, our results indicate that current LLMs exhibit nascent steganographic capabilities. While these capabilities are likely insufficient to bypass well-designed monitors at present, this could change in the future.",
      "url": "https://arxiv.org/abs/2507.02737v1",
      "source": "ArXiv Domain",
      "timestamp": "2025-07-03T15:54:55.000Z",
      "metadata": {
        "arxivId": "2507.02737v1",
        "authors": "Artur Zolkowski, Kei Nishimura-Gasparian, Robert McCarthy, Roland S. Zimmermann, David Lindner",
        "categories": "cs.CR, cs.AI, cs.CL, cs.LG",
        "published": "2025-07-03T15:54:55Z",
        "pdfUrl": "https://arxiv.org/pdf/2507.02737v1.pdf",
        "abstractUrl": "https://arxiv.org/abs/2507.02737v1",
        "rank": 8,
        "domain": "LLM"
      }
    },
    {
      "id": "arxiv-2411.00863v2-1751807424051",
      "title": "Next-Token Prediction Task Assumes Optimal Data Ordering for LLM Training in Proof Generation",
      "description": "In the field of large language model (LLM)-based proof generation, despite extensive training on large datasets such as ArXiv, LLMs still exhibit only modest performance on proving tasks of moderate difficulty. We believe that this is partly due to the widespread presence of suboptimal ordering within the data for each proof used in training. For example, published proofs often follow a purely logical order, where each step logically proceeds from the previous steps based on the deductive rules. This order is designed to facilitate the verification of the proof's soundness, rather than to help people and models learn the discovery process of the proof. In proof generation, we argue that the optimal order for one training data sample occurs when the relevant intermediate supervision for a particular proof step in the proof is always positioned to the left of that proof step. We call such order the intuitively sequential order. We validate our claims using two tasks: intuitionistic propositional logic theorem-proving and digit multiplication. Our experiments verify the order effect and provide support for our explanations. We demonstrate that training is most effective when the proof is in the intuitively sequential order. Moreover, the order effect and the performance gap between models trained on different data orders can be substantial -- with an 11 percent improvement in proof success rate observed in the propositional logic theorem-proving task, between models trained on the optimal order compared to the worst order. Lastly, we define a common type of order issue in advanced math proofs and find that 17.3 percent of theorems with nontrivial proofs in the first two chapters of a widely used graduate-level mathematics textbook suffer from this issue. A detailed list of those proofs is provided in the appendix.",
      "url": "https://arxiv.org/abs/2411.00863v2",
      "source": "ArXiv Domain",
      "timestamp": "2024-10-30T18:00:04.000Z",
      "metadata": {
        "arxivId": "2411.00863v2",
        "authors": "Chenyang An, Shima Imani, Feng Yao, Chengyu Dong, Ali Abbasi, Harsh Shrivastava, Samuel Buss, Jingbo Shang, Gayathri Mahalingam, Pramod Sharma, Maurice Diesendruck",
        "categories": "cs.CL, cs.AI",
        "published": "2024-10-30T18:00:04Z",
        "pdfUrl": "https://arxiv.org/pdf/2411.00863v2.pdf",
        "abstractUrl": "https://arxiv.org/abs/2411.00863v2",
        "rank": 9,
        "domain": "LLM"
      }
    },
    {
      "id": "arxiv-2507.02694v1-1751807424052",
      "title": "Can LLMs Identify Critical Limitations within Scientific Research? A Systematic Evaluation on AI Research Papers",
      "description": "Peer review is fundamental to scientific research, but the growing volume of publications has intensified the challenges of this expertise-intensive process. While LLMs show promise in various scientific tasks, their potential to assist with peer review, particularly in identifying paper limitations, remains understudied. We first present a comprehensive taxonomy of limitation types in scientific research, with a focus on AI. Guided by this taxonomy, for studying limitations, we present LimitGen, the first comprehensive benchmark for evaluating LLMs' capability to support early-stage feedback and complement human peer review. Our benchmark consists of two subsets: LimitGen-Syn, a synthetic dataset carefully created through controlled perturbations of high-quality papers, and LimitGen-Human, a collection of real human-written limitations. To improve the ability of LLM systems to identify limitations, we augment them with literature retrieval, which is essential for grounding identifying limitations in prior scientific findings. Our approach enhances the capabilities of LLM systems to generate limitations in research papers, enabling them to provide more concrete and constructive feedback.",
      "url": "https://arxiv.org/abs/2507.02694v1",
      "source": "ArXiv Domain",
      "timestamp": "2025-07-03T15:04:38.000Z",
      "metadata": {
        "arxivId": "2507.02694v1",
        "authors": "Zhijian Xu, Yilun Zhao, Manasi Patwardhan, Lovekesh Vig, Arman Cohan",
        "categories": "cs.CL",
        "published": "2025-07-03T15:04:38Z",
        "pdfUrl": "https://arxiv.org/pdf/2507.02694v1.pdf",
        "abstractUrl": "https://arxiv.org/abs/2507.02694v1",
        "rank": 10,
        "domain": "LLM"
      }
    },
    {
      "id": "arxiv-2507.02825v1-1751807424594",
      "title": "Establishing Best Practices for Building Rigorous Agentic Benchmarks",
      "description": "Benchmarks are essential for quantitatively tracking progress in AI. As AI agents become increasingly capable, researchers and practitioners have introduced agentic benchmarks to evaluate agents on complex, real-world tasks. These benchmarks typically measure agent capabilities by evaluating task outcomes via specific reward designs. However, we show that many agentic benchmarks have issues task setup or reward design. For example, SWE-bench Verified uses insufficient test cases, while TAU-bench counts empty responses as successful. Such issues can lead to under- or overestimation agents' performance by up to 100% in relative terms. To make agentic evaluation rigorous, we introduce the Agentic Benchmark Checklist (ABC), a set of guidelines that we synthesized from our benchmark-building experience, a survey of best practices, and previously reported issues. When applied to CVE-Bench, a benchmark with a particularly complex evaluation design, ABC reduces the performance overestimation by 33%.",
      "url": "https://arxiv.org/abs/2507.02825v1",
      "source": "ArXiv Domain",
      "timestamp": "2025-07-03T17:35:31.000Z",
      "metadata": {
        "arxivId": "2507.02825v1",
        "authors": "Yuxuan Zhu, Tengjun Jin, Yada Pruksachatkun, Andy Zhang, Shu Liu, Sasha Cui, Sayash Kapoor, Shayne Longpre, Kevin Meng, Rebecca Weiss, Fazl Barez, Rahul Gupta, Jwala Dhamala, Jacob Merizian, Mario Giulianelli, Harry Coppock, Cozmin Ududec, Jasjeet Sekhon, Jacob Steinhardt, Antony Kellerman, Sarah Schwettmann, Matei Zaharia, Ion Stoica, Percy Liang, Daniel Kang",
        "categories": "cs.AI, A.1; I.2.m",
        "published": "2025-07-03T17:35:31Z",
        "pdfUrl": "https://arxiv.org/pdf/2507.02825v1.pdf",
        "abstractUrl": "https://arxiv.org/abs/2507.02825v1",
        "rank": 1,
        "domain": "Agent"
      }
    },
    {
      "id": "arxiv-2507.02773v1-1751807424595",
      "title": "KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot Diagnosis Prediction Using Multi-agent LLMs",
      "description": "Medical diagnosis prediction plays a critical role in disease detection and personalized healthcare. While machine learning (ML) models have been widely adopted for this task, their reliance on supervised training limits their ability to generalize to unseen cases, particularly given the high cost of acquiring large, labeled datasets. Large language models (LLMs) have shown promise in leveraging language abilities and biomedical knowledge for diagnosis prediction. However, they often suffer from hallucinations, lack structured medical reasoning, and produce useless outputs. To address these challenges, we propose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves LLM-based diagnosis prediction through a multi-agent architecture. Our framework consists of a linkage agent for attribute mapping, a retrieval agent for structured knowledge extraction, and a prediction agent that iteratively refines diagnosis predictions. Experimental results demonstrate that KERAP enhances diagnostic reliability efficiently, offering a scalable and interpretable solution for zero-shot medical diagnosis prediction.",
      "url": "https://arxiv.org/abs/2507.02773v1",
      "source": "ArXiv Domain",
      "timestamp": "2025-07-03T16:35:11.000Z",
      "metadata": {
        "arxivId": "2507.02773v1",
        "authors": "Yuzhang Xie, Hejie Cui, Ziyang Zhang, Jiaying Lu, Kai Shu, Fadi Nahab, Xiao Hu, Carl Yang",
        "categories": "cs.AI, cs.LG, cs.MA",
        "published": "2025-07-03T16:35:11Z",
        "pdfUrl": "https://arxiv.org/pdf/2507.02773v1.pdf",
        "abstractUrl": "https://arxiv.org/abs/2507.02773v1",
        "rank": 2,
        "domain": "Agent"
      }
    },
    {
      "id": "arxiv-2507.02755v1-1751807424595",
      "title": "Multi-agent Auditory Scene Analysis",
      "description": "Auditory scene analysis (ASA) aims to retrieve information from the acoustic environment, by carrying out three main tasks: sound source location, separation, and classification. These tasks are traditionally executed with a linear data flow, where the sound sources are first located; then, using their location, each source is separated into its own audio stream; from each of which, information is extracted that is relevant to the application scenario (audio event detection, speaker identification, emotion classification, etc.). However, running these tasks linearly increases the overall response time, while making the last tasks (separation and classification) highly sensitive to errors of the first task (location). A considerable amount of effort and computational complexity has been employed in the state-of-the-art to develop techniques that are the least error-prone possible. However, doing so gives rise to an ASA system that is non-viable in many applications that require a small computational footprint and a low response time, such as bioacoustics, hearing-aid design, search and rescue, human-robot interaction, etc. To this effect, in this work, a multi-agent approach is proposed to carry out ASA where the tasks are run in parallel, with feedback loops between them to compensate for local errors, such as: using the quality of the separation output to correct the location error; and using the classification result to reduce the localization's sensitivity towards interferences. The result is a multi-agent auditory scene analysis (MASA) system that is robust against local errors, without a considerable increase in complexity, and with a low response time. The complete proposed MASA system is provided as a framework that uses open-source tools for sound acquisition and reproduction (JACK) and inter-agent communication (ROS2), allowing users to add their own agents.",
      "url": "https://arxiv.org/abs/2507.02755v1",
      "source": "ArXiv Domain",
      "timestamp": "2025-07-03T16:16:46.000Z",
      "metadata": {
        "arxivId": "2507.02755v1",
        "authors": "Caleb Rascon, Luis Gato-Diaz, Eduardo García-Alarcón",
        "categories": "eess.AS, cs.AI",
        "published": "2025-07-03T16:16:46Z",
        "pdfUrl": "https://arxiv.org/pdf/2507.02755v1.pdf",
        "abstractUrl": "https://arxiv.org/abs/2507.02755v1",
        "rank": 3,
        "domain": "Agent"
      }
    },
    {
      "id": "arxiv-2506.21506v2-1751807424595",
      "title": "Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge",
      "description": "Agentic search such as Deep Research systems-where agents autonomously browse the web, synthesize information, and return comprehensive citation-backed answers-represents a major shift in how users interact with web-scale information. While promising greater efficiency and cognitive offloading, the growing complexity and open-endedness of agentic search have outpaced existing evaluation benchmarks and methodologies, which largely assume short search horizons and static answers. In this paper, we introduce Mind2Web 2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that require real-time web browsing and extensive information synthesis, constructed with over 1000 hours of human labor. To address the challenge of evaluating time-varying and complex answers, we propose a novel Agent-as-a-Judge framework. Our method constructs task-specific judge agents based on a tree-structured rubric design to automatically assess both answer correctness and source attribution. We conduct a comprehensive evaluation of ten frontier agentic search systems and human performance, along with a detailed error analysis to draw insights for future development. The best-performing system, OpenAI Deep Research, can already achieve 50-70% of human performance while spending half the time, highlighting its great potential. Altogether, Mind2Web 2 provides a rigorous foundation for developing and benchmarking the next generation of agentic search systems.",
      "url": "https://arxiv.org/abs/2506.21506v2",
      "source": "ArXiv Domain",
      "timestamp": "2025-06-26T17:32:50.000Z",
      "metadata": {
        "arxivId": "2506.21506v2",
        "authors": "Boyu Gou, Zanming Huang, Yuting Ning, Yu Gu, Michael Lin, Weijian Qi, Andrei Kopanev, Botao Yu, Bernal Jiménez Gutiérrez, Yiheng Shu, Chan Hee Song, Jiaman Wu, Shijie Chen, Hanane Nour Moussa, Tianshu Zhang, Jian Xie, Yifei Li, Tianci Xue, Zeyi Liao, Kai Zhang, Boyuan Zheng, Zhaowei Cai, Viktor Rozgic, Morteza Ziyadi, Huan Sun, Yu Su",
        "categories": "cs.AI, cs.CL",
        "published": "2025-06-26T17:32:50Z",
        "pdfUrl": "https://arxiv.org/pdf/2506.21506v2.pdf",
        "abstractUrl": "https://arxiv.org/abs/2506.21506v2",
        "rank": 4,
        "domain": "Agent"
      }
    },
    {
      "id": "arxiv-2507.02663v1-1751807424595",
      "title": "Think How to Think: Mitigating Overthinking with Autonomous Difficulty Cognition in Large Reasoning Models",
      "description": "Recent Long Reasoning Models(LRMs) have demonstrated remarkable capabilities in handling complex reasoning tasks, but are hindered by excessive overthinking. To explore its essence, our empirical analysis reveals that LRMs are primarily limited to recognizing task properties (i.e., difficulty levels) like humans before solving the problem, leading to a one-size-fits-all reasoning process. Inspired by this, a pressing and natural question emerges: Can we bootstrap such ability to further alleviate the overthinking phenomenon in LRMs? In this paper, we propose Think-How-to-Think (TH2T), a novel two-stage fine-tuning strategy that progressively inspires LRMs' difficulty cognition and redundancy cognition. First, we introduce difficulty-hypnosis in the prefixes of model outputs to intervene in the internal reasoning trajectory. Combined with a heterogeneous short and long reasoning dataset, the trained model enhances its sensitivity to task difficulty, enabling native, differentiated reasoning strategies across various tasks. Second, we further extend redundancy-hypnosis to the internal reasoning process, guiding the model to identify redundant structures within the reasoning steps and generate more concise reasoning outputs. Experiments on 7B/14B/32B models demonstrate that TH2T significantly reduces inference costs (more than 70% on easy tasks and 40% on hard tasks) while maintaining performance stability. The resulting outputs exhibit clear difficulty-aware capabilities and reduced redundancy (e.g., reflection).",
      "url": "https://arxiv.org/abs/2507.02663v1",
      "source": "ArXiv Domain",
      "timestamp": "2025-07-03T14:24:26.000Z",
      "metadata": {
        "arxivId": "2507.02663v1",
        "authors": "Yongjiang Liu, Haoxi Li, Xiaosong Ma, Jie Zhang, Song Guo",
        "categories": "cs.AI",
        "published": "2025-07-03T14:24:26Z",
        "pdfUrl": "https://arxiv.org/pdf/2507.02663v1.pdf",
        "abstractUrl": "https://arxiv.org/abs/2507.02663v1",
        "rank": 5,
        "domain": "Agent"
      }
    },
    {
      "id": "arxiv-2507.02660v1-1751807424595",
      "title": "Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification",
      "description": "Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is their development process. Hardware design verification entails a methodical and disciplined approach to the planning, development, execution, and sign-off of functionally correct hardware designs. This tedious process requires significant effort and time to ensure a bug-free tape-out. The field of Natural Language Processing has undergone a significant transformation with the advent of Large Language Models (LLMs). These powerful models, often referred to as Generative AI (GenAI), have revolutionized how machines understand and generate human language, enabling unprecedented advancements in a wide array of applications, including hardware design verification. This paper presents an agentic AI-based approach to hardware design verification, which empowers AI agents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage in a more dynamic, iterative, and self-reflective process, ultimately performing end-to-end hardware design and verification. This methodology is evaluated on five open-source designs, achieving over 95% coverage with reduced verification time while demonstrating superior performance, adaptability, and configurability.",
      "url": "https://arxiv.org/abs/2507.02660v1",
      "source": "ArXiv Domain",
      "timestamp": "2025-07-03T14:20:57.000Z",
      "metadata": {
        "arxivId": "2507.02660v1",
        "authors": "Deepak Narayan Gadde, Keerthan Kopparam Radhakrishna, Vaisakh Naduvodi Viswambharan, Aman Kumar, Djones Lettnin, Wolfgang Kunz, Sebastian Simon",
        "categories": "cs.AI, cs.AR",
        "published": "2025-07-03T14:20:57Z",
        "pdfUrl": "https://arxiv.org/pdf/2507.02660v1.pdf",
        "abstractUrl": "https://arxiv.org/abs/2507.02660v1",
        "rank": 6,
        "domain": "Agent"
      }
    },
    {
      "id": "arxiv-2507.02616v1-1751807424595",
      "title": "DynamiCare: A Dynamic Multi-Agent Framework for Interactive and Open-Ended Medical Decision-Making",
      "description": "The rise of Large Language Models (LLMs) has enabled the development of specialized AI agents with domain-specific reasoning and interaction capabilities, particularly in healthcare. While recent frameworks simulate medical decision-making, they largely focus on single-turn tasks where a doctor agent receives full case information upfront -- diverging from the real-world diagnostic process, which is inherently uncertain, interactive, and iterative. In this paper, we introduce MIMIC-Patient, a structured dataset built from the MIMIC-III electronic health records (EHRs), designed to support dynamic, patient-level simulations. Building on this, we propose DynamiCare, a novel dynamic multi-agent framework that models clinical diagnosis as a multi-round, interactive loop, where a team of specialist agents iteratively queries the patient system, integrates new information, and dynamically adapts its composition and strategy. We demonstrate the feasibility and effectiveness of DynamiCare through extensive experiments, establishing the first benchmark for dynamic clinical decision-making with LLM-powered agents.",
      "url": "https://arxiv.org/abs/2507.02616v1",
      "source": "ArXiv Domain",
      "timestamp": "2025-07-03T13:43:10.000Z",
      "metadata": {
        "arxivId": "2507.02616v1",
        "authors": "Tianqi Shang, Weiqing He, Charles Zheng, Lingyao Li, Li Shen, Bingxin Zhao",
        "categories": "cs.AI",
        "published": "2025-07-03T13:43:10Z",
        "pdfUrl": "https://arxiv.org/pdf/2507.02616v1.pdf",
        "abstractUrl": "https://arxiv.org/abs/2507.02616v1",
        "rank": 7,
        "domain": "Agent"
      }
    },
    {
      "id": "arxiv-2507.02592v1-1751807424595",
      "title": "WebSailor: Navigating Super-human Reasoning for Web Agent",
      "description": "Transcending human cognitive limitations represents a critical frontier in LLM training. Proprietary agentic systems like DeepResearch have demonstrated superhuman capabilities on extremely complex information-seeking benchmarks such as BrowseComp, a feat previously unattainable. We posit that their success hinges on a sophisticated reasoning pattern absent in open-source models: the ability to systematically reduce extreme uncertainty when navigating vast information landscapes. Based on this insight, we introduce WebSailor, a complete post-training methodology designed to instill this crucial capability. Our approach involves generating novel, high-uncertainty tasks through structured sampling and information obfuscation, RFT cold start, and an efficient agentic RL training algorithm, Duplicating Sampling Policy Optimization (DUPO). With this integrated pipeline, WebSailor significantly outperforms all opensource agents in complex information-seeking tasks, matching proprietary agents' performance and closing the capability gap.",
      "url": "https://arxiv.org/abs/2507.02592v1",
      "source": "ArXiv Domain",
      "timestamp": "2025-07-03T12:59:07.000Z",
      "metadata": {
        "arxivId": "2507.02592v1",
        "authors": "Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, Weizhou Shen, Junkai Zhang, Dingchu Zhang, Xixi Wu, Yong Jiang, Ming Yan, Pengjun Xie, Fei Huang, Jingren Zhou",
        "categories": "cs.CL, cs.AI",
        "published": "2025-07-03T12:59:07Z",
        "pdfUrl": "https://arxiv.org/pdf/2507.02592v1.pdf",
        "abstractUrl": "https://arxiv.org/abs/2507.02592v1",
        "rank": 8,
        "domain": "Agent"
      }
    },
    {
      "id": "arxiv-2507.02554v1-1751807424596",
      "title": "AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench",
      "description": "AI research agents are demonstrating great potential to accelerate scientific progress by automating the design, implementation, and training of machine learning models. We focus on methods for improving agents' performance on MLE-bench, a challenging benchmark where agents compete in Kaggle competitions to solve real-world machine learning problems. We formalize AI research agents as search policies that navigate a space of candidate solutions, iteratively modifying them using operators. By designing and systematically varying different operator sets and search policies (Greedy, MCTS, Evolutionary), we show that their interplay is critical for achieving high performance. Our best pairing of search strategy and operator set achieves a state-of-the-art result on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from 39.6% to 47.7%. Our investigation underscores the importance of jointly considering the search strategy, operator design, and evaluation methodology in advancing automated machine learning.",
      "url": "https://arxiv.org/abs/2507.02554v1",
      "source": "ArXiv Domain",
      "timestamp": "2025-07-03T11:59:15.000Z",
      "metadata": {
        "arxivId": "2507.02554v1",
        "authors": "Edan Toledo, Karen Hambardzumyan, Martin Josifoski, Rishi Hazra, Nicolas Baldwin, Alexis Audran-Reiss, Michael Kuchnik, Despoina Magka, Minqi Jiang, Alisia Maria Lupidi, Andrei Lupu, Roberta Raileanu, Kelvin Niu, Tatiana Shavrina, Jean-Christophe Gagnon-Audet, Michael Shvartsman, Shagun Sodhani, Alexander H. Miller, Abhishek Charnalia, Derek Dunfield, Carole-Jean Wu, Pontus Stenetorp, Nicola Cancedda, Jakob Nicolaus Foerster, Yoram Bachrach",
        "categories": "cs.AI, cs.LG",
        "published": "2025-07-03T11:59:15Z",
        "pdfUrl": "https://arxiv.org/pdf/2507.02554v1.pdf",
        "abstractUrl": "https://arxiv.org/abs/2507.02554v1",
        "rank": 9,
        "domain": "Agent"
      }
    },
    {
      "id": "arxiv-2506.22355v2-1751807424596",
      "title": "Embodied AI Agents: Modeling the World",
      "description": "This paper describes our research on AI agents embodied in visual, virtual or physical forms, enabling them to interact with both users and their environments. These agents, which include virtual avatars, wearable devices, and robots, are designed to perceive, learn and act within their surroundings, which makes them more similar to how humans learn and interact with the environments as compared to disembodied agents. We propose that the development of world models is central to reasoning and planning of embodied AI agents, allowing these agents to understand and predict their environment, to understand user intentions and social contexts, thereby enhancing their ability to perform complex tasks autonomously. World modeling encompasses the integration of multimodal perception, planning through reasoning for action and control, and memory to create a comprehensive understanding of the physical world. Beyond the physical world, we also propose to learn the mental world model of users to enable better human-agent collaboration.",
      "url": "https://arxiv.org/abs/2506.22355v2",
      "source": "ArXiv Domain",
      "timestamp": "2025-06-27T16:05:34.000Z",
      "metadata": {
        "arxivId": "2506.22355v2",
        "authors": "Pascale Fung, Yoram Bachrach, Asli Celikyilmaz, Kamalika Chaudhuri, Delong Chen, Willy Chung, Emmanuel Dupoux, Hongyu Gong, Hervé Jégou, Alessandro Lazaric, Arjun Majumdar, Andrea Madotto, Franziska Meier, Florian Metze, Louis-Philippe Morency, Théo Moutakanni, Juan Pino, Basile Terver, Joseph Tighe, Paden Tomasello, Jitendra Malik",
        "categories": "cs.AI",
        "published": "2025-06-27T16:05:34Z",
        "pdfUrl": "https://arxiv.org/pdf/2506.22355v2.pdf",
        "abstractUrl": "https://arxiv.org/abs/2506.22355v2",
        "rank": 10,
        "domain": "Agent"
      }
    },
    {
      "id": "arxiv-2306.13840v4-1751807425074",
      "title": "Beyond Scale: The Diversity Coefficient as a Data Quality Metric for Variability in Natural Language Data",
      "description": "Current trends in pre-training Large Language Models (LLMs) primarily focus on the scaling of model and dataset size. While the quality of pre-training data is considered an important factor for training powerful LLMs, it remains a nebulous concept that has not been rigorously characterized. To this end, we propose a formalization of one key aspect of data quality -- measuring the variability of natural language data -- specifically via a measure we call the diversity coefficient. Our empirical analysis shows that the proposed diversity coefficient aligns with the intuitive properties of diversity and variability, e.g., it increases as the number of latent concepts increases. Then, we measure the diversity coefficient of publicly available pre-training datasets and demonstrate that their formal diversity is high compared to theoretical lower and upper bounds. Finally, we conduct a comprehensive set of controlled interventional experiments with GPT-2 and LLaMAv2 that demonstrate the diversity coefficient of pre-training data characterizes useful aspects of downstream model evaluation performance -- totaling 44 models of various sizes (51M to 7B parameters). We conclude that our formal notion of diversity is an important aspect of data quality that captures variability and causally leads to improved evaluation performance.",
      "url": "https://arxiv.org/abs/2306.13840v4",
      "source": "ArXiv Domain",
      "timestamp": "2023-06-24T02:25:56.000Z",
      "metadata": {
        "arxivId": "2306.13840v4",
        "authors": "Brando Miranda, Alycia Lee, Sudharsan Sundar, Allison Casasola, Rylan Schaeffer, Elyas Obbad, Sanmi Koyejo",
        "categories": "cs.CL, cs.AI, cs.LG, cs.NE",
        "published": "2023-06-24T02:25:56Z",
        "pdfUrl": "https://arxiv.org/pdf/2306.13840v4.pdf",
        "abstractUrl": "https://arxiv.org/abs/2306.13840v4",
        "rank": 1,
        "domain": "NLP"
      }
    },
    {
      "id": "arxiv-2507.00297v1-1751807425075",
      "title": "Natural language processing for African languages",
      "description": "Recent advances in word embeddings and language models use large-scale, unlabelled data and self-supervised learning to boost NLP performance. Multilingual models, often trained on web-sourced data like Wikipedia, face challenges: few low-resource languages are included, their data is often noisy, and lack of labeled datasets makes it hard to evaluate performance outside high-resource languages like English. In this dissertation, we focus on languages spoken in Sub-Saharan Africa where all the indigenous languages in this region can be regarded as low-resourced in terms of the availability of labelled data for NLP tasks and unlabelled data found on the web. We analyse the noise in the publicly available corpora, and curate a high-quality corpus, demonstrating that the quality of semantic representations learned in word embeddings does not only depend on the amount of data but on the quality of pre-training data. We demonstrate empirically the limitations of word embeddings, and the opportunities the multilingual pre-trained language model (PLM) offers especially for languages unseen during pre-training and low-resource scenarios. We further study how to adapt and specialize multilingual PLMs to unseen African languages using a small amount of monolingual texts. To address the under-representation of the African languages in NLP research, we developed large scale human-annotated labelled datasets for 21 African languages in two impactful NLP tasks: named entity recognition and machine translation. We conduct an extensive empirical evaluation using state-of-the-art methods across supervised, weakly-supervised, and transfer learning settings.",
      "url": "https://arxiv.org/abs/2507.00297v1",
      "source": "ArXiv Domain",
      "timestamp": "2025-06-30T22:26:36.000Z",
      "metadata": {
        "arxivId": "2507.00297v1",
        "authors": "David Ifeoluwa Adelani",
        "categories": "cs.CL, cs.AI",
        "published": "2025-06-30T22:26:36Z",
        "pdfUrl": "https://arxiv.org/pdf/2507.00297v1.pdf",
        "abstractUrl": "https://arxiv.org/abs/2507.00297v1",
        "rank": 2,
        "domain": "NLP"
      }
    },
    {
      "id": "arxiv-2506.10558v2-1751807425075",
      "title": "StepProof: Step-by-step verification of natural language mathematical proofs",
      "description": "Interactive theorem provers (ITPs) are powerful tools for the formal verification of mathematical proofs down to the axiom level. However, their lack of a natural language interface remains a significant limitation. Recent advancements in large language models (LLMs) have enhanced the understanding of natural language inputs, paving the way for autoformalization - the process of translating natural language proofs into formal proofs that can be verified. Despite these advancements, existing autoformalization approaches are limited to verifying complete proofs and lack the capability for finer, sentence-level verification. To address this gap, we propose StepProof, a novel autoformalization method designed for granular, step-by-step verification. StepProof breaks down complete proofs into multiple verifiable subproofs, enabling sentence-level verification. Experimental results demonstrate that StepProof significantly improves proof success rates and efficiency compared to traditional methods. Additionally, we found that minor manual adjustments to the natural language proofs, tailoring them for step-level verification, further enhanced StepProof's performance in autoformalization.",
      "url": "https://arxiv.org/abs/2506.10558v2",
      "source": "ArXiv Domain",
      "timestamp": "2025-06-12T10:31:23.000Z",
      "metadata": {
        "arxivId": "2506.10558v2",
        "authors": "Xiaolin Hu, Qinghua Zhou, Bogdan Grechuk, Ivan Y. Tyukin",
        "categories": "cs.LO, cs.AI",
        "published": "2025-06-12T10:31:23Z",
        "pdfUrl": "https://arxiv.org/pdf/2506.10558v2.pdf",
        "abstractUrl": "https://arxiv.org/abs/2506.10558v2",
        "rank": 3,
        "domain": "NLP"
      }
    },
    {
      "id": "arxiv-2507.01059v1-1751807425075",
      "title": "Automated Vehicles Should be Connected with Natural Language",
      "description": "Multi-agent collaborative driving promises improvements in traffic safety and efficiency through collective perception and decision making. However, existing communication media -- including raw sensor data, neural network features, and perception results -- suffer limitations in bandwidth efficiency, information completeness, and agent interoperability. Moreover, traditional approaches have largely ignored decision-level fusion, neglecting critical dimensions of collaborative driving. In this paper we argue that addressing these challenges requires a transition from purely perception-oriented data exchanges to explicit intent and reasoning communication using natural language. Natural language balances semantic density and communication bandwidth, adapts flexibly to real-time conditions, and bridges heterogeneous agent platforms. By enabling the direct communication of intentions, rationales, and decisions, it transforms collaborative driving from reactive perception-data sharing into proactive coordination, advancing safety, efficiency, and transparency in intelligent transportation systems.",
      "url": "https://arxiv.org/abs/2507.01059v1",
      "source": "ArXiv Domain",
      "timestamp": "2025-06-29T16:41:19.000Z",
      "metadata": {
        "arxivId": "2507.01059v1",
        "authors": "Xiangbo Gao, Keshu Wu, Hao Zhang, Kexin Tian, Yang Zhou, Zhengzhong Tu",
        "categories": "cs.MA, cs.AI, cs.CL, cs.CV, cs.RO",
        "published": "2025-06-29T16:41:19Z",
        "pdfUrl": "https://arxiv.org/pdf/2507.01059v1.pdf",
        "abstractUrl": "https://arxiv.org/abs/2507.01059v1",
        "rank": 4,
        "domain": "NLP"
      }
    },
    {
      "id": "arxiv-2404.19543v2-1751807425075",
      "title": "RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing",
      "description": "Large Language Models (LLMs) have catalyzed significant advancements in Natural Language Processing (NLP), yet they encounter challenges such as hallucination and the need for domain-specific knowledge. To mitigate these, recent methodologies have integrated information retrieved from external resources with LLMs, substantially enhancing their performance across NLP tasks. This survey paper addresses the absence of a comprehensive overview on Retrieval-Augmented Language Models (RALMs), both Retrieval-Augmented Generation (RAG) and Retrieval-Augmented Understanding (RAU), providing an in-depth examination of their paradigm, evolution, taxonomy, and applications. The paper discusses the essential components of RALMs, including Retrievers, Language Models, and Augmentations, and how their interactions lead to diverse model structures and applications. RALMs demonstrate utility in a spectrum of tasks, from translation and dialogue systems to knowledge-intensive applications. The survey includes several evaluation methods of RALMs, emphasizing the importance of robustness, accuracy, and relevance in their assessment. It also acknowledges the limitations of RALMs, particularly in retrieval quality and computational efficiency, offering directions for future research. In conclusion, this survey aims to offer a structured insight into RALMs, their potential, and the avenues for their future development in NLP. The paper is supplemented with a Github Repository containing the surveyed works and resources for further study: https://github.com/2471023025/RALM_Survey.",
      "url": "https://arxiv.org/abs/2404.19543v2",
      "source": "ArXiv Domain",
      "timestamp": "2024-04-30T13:14:51.000Z",
      "metadata": {
        "arxivId": "2404.19543v2",
        "authors": "Yucheng Hu, Yuxing Lu",
        "categories": "cs.CL, cs.AI",
        "published": "2024-04-30T13:14:51Z",
        "pdfUrl": "https://arxiv.org/pdf/2404.19543v2.pdf",
        "abstractUrl": "https://arxiv.org/abs/2404.19543v2",
        "rank": 5,
        "domain": "NLP"
      }
    },
    {
      "id": "arxiv-2506.20988v1-1751807425075",
      "title": "Segment Anything in Pathology Images with Natural Language",
      "description": "Pathology image segmentation is crucial in computational pathology for analyzing histological features relevant to cancer diagnosis and prognosis. However, current methods face major challenges in clinical applications due to limited annotated data and restricted category definitions. To address these limitations, we propose PathSegmentor, the first text-prompted segmentation foundation model designed specifically for pathology images. We also introduce PathSeg , the largest and most comprehensive dataset for pathology segmentation, built from 17 public sources and containing 275k image-mask-label triples across 160 diverse categories. With PathSegmentor, users can perform semantic segmentation using natural language prompts, eliminating the need for laborious spatial inputs such as points or boxes. Extensive experiments demonstrate that PathSegmentor outperforms specialized models with higher accuracy and broader applicability, while maintaining a compact architecture. It significantly surpasses existing spatial- and text-prompted models by 0.145 and 0.429 in overall Dice scores, respectively, showing strong robustness in segmenting complex structures and generalizing to external datasets. Moreover, PathSegmentor's outputs enhance the interpretability of diagnostic models through feature importance estimation and imaging biomarker discovery, offering pathologists evidence-based support for clinical decision-making. This work advances the development of explainable AI in precision oncology.",
      "url": "https://arxiv.org/abs/2506.20988v1",
      "source": "ArXiv Domain",
      "timestamp": "2025-06-26T04:01:40.000Z",
      "metadata": {
        "arxivId": "2506.20988v1",
        "authors": "Zhixuan Chen, Junlin Hou, Liqi Lin, Yihui Wang, Yequan Bie, Xi Wang, Yanning Zhou, Ronald Cheong Kin Chan, Hao Chen",
        "categories": "cs.CV, cs.AI",
        "published": "2025-06-26T04:01:40Z",
        "pdfUrl": "https://arxiv.org/pdf/2506.20988v1.pdf",
        "abstractUrl": "https://arxiv.org/abs/2506.20988v1",
        "rank": 6,
        "domain": "NLP"
      }
    },
    {
      "id": "arxiv-2506.17700v1-1751807425076",
      "title": "The Evolution of Natural Language Processing: How Prompt Optimization and Language Models are Shaping the Future",
      "description": "Large Language Models (LLMs) have revolutionized the field of Natural Language Processing (NLP) by automating traditional labor-intensive tasks and consequently accelerated the development of computer-aided applications. As researchers continue to advance this field with the introduction of novel language models and more efficient training/finetuning methodologies, the idea of prompt engineering and subsequent optimization strategies with LLMs has emerged as a particularly impactful trend to yield a substantial performance boost across diverse NLP tasks. To best of our knowledge numerous review articles have explored prompt engineering, however, a critical gap exists in comprehensive analyses of prompt optimization strategies. To bridge this gap this paper provides unique and comprehensive insights about the potential of diverse prompt optimization strategies. It analyzes their underlying working paradigms and based on these principles, categorizes them into 11 distinct classes. Moreover, the paper provides details about various NLP tasks where these prompt optimization strategies have been employed, along with details of different LLMs and benchmark datasets used for evaluation. This comprehensive compilation lays a robust foundation for future comparative studies and enables rigorous assessment of prompt optimization and LLM-based predictive pipelines under consistent experimental settings: a critical need in the current landscape. Ultimately, this research will centralize diverse strategic knowledge to facilitate the adaptation of existing prompt optimization strategies for development of innovative predictors across unexplored tasks.",
      "url": "https://arxiv.org/abs/2506.17700v1",
      "source": "ArXiv Domain",
      "timestamp": "2025-06-21T12:25:37.000Z",
      "metadata": {
        "arxivId": "2506.17700v1",
        "authors": "Summra Saleem, Muhammad Nabeel Asim, Shaista Zulfiqar, Andreas Dengel",
        "categories": "cs.CL, cs.AI",
        "published": "2025-06-21T12:25:37Z",
        "pdfUrl": "https://arxiv.org/pdf/2506.17700v1.pdf",
        "abstractUrl": "https://arxiv.org/abs/2506.17700v1",
        "rank": 7,
        "domain": "NLP"
      }
    },
    {
      "id": "arxiv-2402.07051v2-1751807425076",
      "title": "$L^*LM$: Learning Automata from Examples using Natural Language Oracles",
      "description": "Expert demonstrations have proven an easy way to indirectly specify complex tasks. Recent algorithms even support extracting unambiguous formal specifications, e.g. deterministic finite automata (DFA), from demonstrations. Unfortunately, these techniques are generally not sample efficient. In this work, we introduce $L^*LM$, an algorithm for learning DFAs from both demonstrations and natural language. Due to the expressivity of natural language, we observe a significant improvement in the data efficiency of learning DFAs from expert demonstrations. Technically, $L^*LM$ leverages large language models to answer membership queries about the underlying task. This is then combined with recent techniques for transforming learning from demonstrations into a sequence of labeled example learning problems. In our experiments, we observe the two modalities complement each other, yielding a powerful few-shot learner.",
      "url": "https://arxiv.org/abs/2402.07051v2",
      "source": "ArXiv Domain",
      "timestamp": "2024-02-10T21:46:34.000Z",
      "metadata": {
        "arxivId": "2402.07051v2",
        "authors": "Marcell Vazquez-Chanlatte, Karim Elmaaroufi, Stefan J. Witwicki, Matei Zaharia, Sanjit A. Seshia",
        "categories": "cs.LG, cs.AI, cs.CL, cs.FL",
        "published": "2024-02-10T21:46:34Z",
        "pdfUrl": "https://arxiv.org/pdf/2402.07051v2.pdf",
        "abstractUrl": "https://arxiv.org/abs/2402.07051v2",
        "rank": 8,
        "domain": "NLP"
      }
    },
    {
      "id": "arxiv-2502.07527v3-1751807425076",
      "title": "Nature Language Model: Deciphering the Language of Nature for Scientific Discovery",
      "description": "Foundation models have revolutionized natural language processing and artificial intelligence, significantly enhancing how machines comprehend and generate human languages. Inspired by the success of these foundation models, researchers have developed foundation models for individual scientific domains, including small molecules, materials, proteins, DNA, RNA and even cells. However, these models are typically trained in isolation, lacking the ability to integrate across different scientific domains. Recognizing that entities within these domains can all be represented as sequences, which together form the \"language of nature\", we introduce Nature Language Model (NatureLM), a sequence-based science foundation model designed for scientific discovery. Pre-trained with data from multiple scientific domains, NatureLM offers a unified, versatile model that enables various applications including: (i) generating and optimizing small molecules, proteins, RNA, and materials using text instructions; (ii) cross-domain generation/design, such as protein-to-molecule and protein-to-RNA generation; and (iii) top performance across different domains, matching or surpassing state-of-the-art specialist models. NatureLM offers a promising generalist approach for various scientific tasks, including drug discovery (hit generation/optimization, ADMET optimization, synthesis), novel material design, and the development of therapeutic proteins or nucleotides. We have developed NatureLM models in different sizes (1 billion, 8 billion, and 46.7 billion parameters) and observed a clear improvement in performance as the model size increases.",
      "url": "https://arxiv.org/abs/2502.07527v3",
      "source": "ArXiv Domain",
      "timestamp": "2025-02-11T13:08:03.000Z",
      "metadata": {
        "arxivId": "2502.07527v3",
        "authors": "Yingce Xia, Peiran Jin, Shufang Xie, Liang He, Chuan Cao, Renqian Luo, Guoqing Liu, Yue Wang, Zequn Liu, Yuan-Jyue Chen, Zekun Guo, Yeqi Bai, Pan Deng, Yaosen Min, Ziheng Lu, Hongxia Hao, Han Yang, Jielan Li, Chang Liu, Jia Zhang, Jianwei Zhu, Ran Bi, Kehan Wu, Wei Zhang, Kaiyuan Gao, Qizhi Pei, Qian Wang, Xixian Liu, Yanting Li, Houtian Zhu, Yeqing Lu, Mingqian Ma, Zun Wang, Tian Xie, Krzysztof Maziarz, Marwin Segler, Zhao Yang, Zilong Chen, Yu Shi, Shuxin Zheng, Lijun Wu, Chen Hu, Peggy Dai, Tie-Yan Liu, Haiguang Liu, Tao Qin",
        "categories": "cs.AI, cs.LG",
        "published": "2025-02-11T13:08:03Z",
        "pdfUrl": "https://arxiv.org/pdf/2502.07527v3.pdf",
        "abstractUrl": "https://arxiv.org/abs/2502.07527v3",
        "rank": 9,
        "domain": "NLP"
      }
    },
    {
      "id": "arxiv-2404.12041v3-1751807425076",
      "title": "A Survey of Automatic Hallucination Evaluation on Natural Language Generation",
      "description": "The proliferation of Large Language Models (LLMs) has introduced a critical challenge: accurate hallucination evaluation that ensures model reliability. While Automatic Hallucination Evaluation (AHE) has emerged as essential, the field suffers from methodological fragmentation, hindering both theoretical understanding and practical advancement. This survey addresses this critical gap through a comprehensive analysis of 74 evaluation methods, revealing that 74% specifically target LLMs, a paradigm shift that demands new evaluation frameworks. We formulate a unified evaluation pipeline encompassing datasets and benchmarks, evidence collection strategies, and comparison mechanisms, systematically documenting the evolution from pre-LLM to post-LLM methodologies. Beyond taxonomical organization, we identify fundamental limitations in current approaches and their implications for real-world deployment. To guide future research, we delineate key challenges and propose strategic directions, including enhanced interpretability mechanisms and integration of application-specific evaluation criteria, ultimately providing a roadmap for developing more robust and practical hallucination evaluation systems.",
      "url": "https://arxiv.org/abs/2404.12041v3",
      "source": "ArXiv Domain",
      "timestamp": "2024-04-18T09:52:18.000Z",
      "metadata": {
        "arxivId": "2404.12041v3",
        "authors": "Siya Qi, Lin Gui, Yulan He, Zheng Yuan",
        "categories": "cs.CL, cs.AI",
        "published": "2024-04-18T09:52:18Z",
        "pdfUrl": "https://arxiv.org/pdf/2404.12041v3.pdf",
        "abstractUrl": "https://arxiv.org/abs/2404.12041v3",
        "rank": 10,
        "domain": "NLP"
      }
    },
    {
      "id": "arxiv-2507.02863v1-1751807425419",
      "title": "Point3R: Streaming 3D Reconstruction with Explicit Spatial Pointer Memory",
      "description": "Dense 3D scene reconstruction from an ordered sequence or unordered image collections is a critical step when bringing research in computer vision into practical scenarios. Following the paradigm introduced by DUSt3R, which unifies an image pair densely into a shared coordinate system, subsequent methods maintain an implicit memory to achieve dense 3D reconstruction from more images. However, such implicit memory is limited in capacity and may suffer from information loss of earlier frames. We propose Point3R, an online framework targeting dense streaming 3D reconstruction. To be specific, we maintain an explicit spatial pointer memory directly associated with the 3D structure of the current scene. Each pointer in this memory is assigned a specific 3D position and aggregates scene information nearby in the global coordinate system into a changing spatial feature. Information extracted from the latest frame interacts explicitly with this pointer memory, enabling dense integration of the current observation into the global coordinate system. We design a 3D hierarchical position embedding to promote this interaction and design a simple yet effective fusion mechanism to ensure that our pointer memory is uniform and efficient. Our method achieves competitive or state-of-the-art performance on various tasks with low training costs. Code is available at: https://github.com/YkiWu/Point3R.",
      "url": "https://arxiv.org/abs/2507.02863v1",
      "source": "ArXiv Domain",
      "timestamp": "2025-07-03T17:59:56.000Z",
      "metadata": {
        "arxivId": "2507.02863v1",
        "authors": "Yuqi Wu, Wenzhao Zheng, Jie Zhou, Jiwen Lu",
        "categories": "cs.CV, cs.AI, cs.LG",
        "published": "2025-07-03T17:59:56Z",
        "pdfUrl": "https://arxiv.org/pdf/2507.02863v1.pdf",
        "abstractUrl": "https://arxiv.org/abs/2507.02863v1",
        "rank": 1,
        "domain": "AI"
      }
    },
    {
      "id": "arxiv-2507.02861v1-1751807425419",
      "title": "LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans",
      "description": "We propose LiteReality, a novel pipeline that converts RGB-D scans of indoor environments into compact, realistic, and interactive 3D virtual replicas. LiteReality not only reconstructs scenes that visually resemble reality but also supports key features essential for graphics pipelines -- such as object individuality, articulation, high-quality physically based rendering materials, and physically based interaction. At its core, LiteReality first performs scene understanding and parses the results into a coherent 3D layout and objects with the help of a structured scene graph. It then reconstructs the scene by retrieving the most visually similar 3D artist-crafted models from a curated asset database. Next, the Material Painting module enhances realism by recovering high-quality, spatially varying materials. Finally, the reconstructed scene is integrated into a simulation engine with basic physical properties to enable interactive behavior. The resulting scenes are compact, editable, and fully compatible with standard graphics pipelines, making them suitable for applications in AR/VR, gaming, robotics, and digital twins. In addition, LiteReality introduces a training-free object retrieval module that achieves state-of-the-art similarity performance on the Scan2CAD benchmark, along with a robust material painting module capable of transferring appearances from images of any style to 3D assets -- even under severe misalignment, occlusion, and poor lighting. We demonstrate the effectiveness of LiteReality on both real-life scans and public datasets. Project page: https://litereality.github.io; Video: https://www.youtube.com/watch?v=ecK9m3LXg2c",
      "url": "https://arxiv.org/abs/2507.02861v1",
      "source": "ArXiv Domain",
      "timestamp": "2025-07-03T17:59:55.000Z",
      "metadata": {
        "arxivId": "2507.02861v1",
        "authors": "Zhening Huang, Xiaoyang Wu, Fangcheng Zhong, Hengshuang Zhao, Matthias Nießner, Joan Lasenby",
        "categories": "cs.CV, cs.AI, cs.GR",
        "published": "2025-07-03T17:59:55Z",
        "pdfUrl": "https://arxiv.org/pdf/2507.02861v1.pdf",
        "abstractUrl": "https://arxiv.org/abs/2507.02861v1",
        "rank": 2,
        "domain": "AI"
      }
    },
    {
      "id": "arxiv-2507.02858v1-1751807425419",
      "title": "Requirements Elicitation Follow-Up Question Generation",
      "description": "Interviews are a widely used technique in eliciting requirements to gather stakeholder needs, preferences, and expectations for a software system. Effective interviewing requires skilled interviewers to formulate appropriate interview questions in real time while facing multiple challenges, including lack of familiarity with the domain, excessive cognitive load, and information overload that hinders how humans process stakeholders' speech. Recently, large language models (LLMs) have exhibited state-of-the-art performance in multiple natural language processing tasks, including text summarization and entailment. To support interviewers, we investigate the application of GPT-4o to generate follow-up interview questions during requirements elicitation by building on a framework of common interviewer mistake types. In addition, we describe methods to generate questions based on interviewee speech. We report a controlled experiment to evaluate LLM-generated and human-authored questions with minimal guidance, and a second controlled experiment to evaluate the LLM-generated questions when generation is guided by interviewer mistake types. Our findings demonstrate that, for both experiments, the LLM-generated questions are no worse than the human-authored questions with respect to clarity, relevancy, and informativeness. In addition, LLM-generated questions outperform human-authored questions when guided by common mistakes types. This highlights the potential of using LLMs to help interviewers improve the quality and ease of requirements elicitation interviews in real time.",
      "url": "https://arxiv.org/abs/2507.02858v1",
      "source": "ArXiv Domain",
      "timestamp": "2025-07-03T17:59:04.000Z",
      "metadata": {
        "arxivId": "2507.02858v1",
        "authors": "Yuchen Shen, Anmol Singhal, Travis Breaux",
        "categories": "cs.SE, cs.CL",
        "published": "2025-07-03T17:59:04Z",
        "pdfUrl": "https://arxiv.org/pdf/2507.02858v1.pdf",
        "abstractUrl": "https://arxiv.org/abs/2507.02858v1",
        "rank": 3,
        "domain": "AI"
      }
    },
    {
      "id": "arxiv-2507.02856v1-1751807425420",
      "title": "Answer Matching Outperforms Multiple Choice for Language Model Evaluation",
      "description": "Multiple choice benchmarks have long been the workhorse of language model evaluation because grading multiple choice is objective and easy to automate. However, we show multiple choice questions from popular benchmarks can often be answered without even seeing the question. These shortcuts arise from a fundamental limitation of discriminative evaluation not shared by evaluations of the model's free-form, generative answers. Until recently, there appeared to be no viable, scalable alternative to multiple choice--but, we show that this has changed. We consider generative evaluation via what we call answer matching: Give the candidate model the question without the options, have it generate a free-form response, then use a modern language model with the reference answer to determine if the response matches the reference. To compare the validity of different evaluation strategies, we annotate MMLU-Pro and GPQA-Diamond to obtain human grading data, and measure the agreement of each evaluation approach. We find answer matching using recent models--even small ones--achieves near-perfect agreement, in the range of inter-annotator agreement. In contrast, both multiple choice evaluation and using LLM-as-a-judge without reference answers aligns poorly with human grading. Improving evaluations via answer matching is not merely a conceptual concern: the rankings of several models change significantly when evaluating their free-form responses with answer matching. In light of these findings, we discuss how to move the evaluation ecosystem from multiple choice to answer matching.",
      "url": "https://arxiv.org/abs/2507.02856v1",
      "source": "ArXiv Domain",
      "timestamp": "2025-07-03T17:59:02.000Z",
      "metadata": {
        "arxivId": "2507.02856v1",
        "authors": "Nikhil Chandak, Shashwat Goel, Ameya Prabhu, Moritz Hardt, Jonas Geiping",
        "categories": "cs.CL, cs.AI, cs.LG",
        "published": "2025-07-03T17:59:02Z",
        "pdfUrl": "https://arxiv.org/pdf/2507.02856v1.pdf",
        "abstractUrl": "https://arxiv.org/abs/2507.02856v1",
        "rank": 4,
        "domain": "AI"
      }
    },
    {
      "id": "arxiv-2507.02855v1-1751807425420",
      "title": "Subtyping in DHOL -- Extended preprint",
      "description": "The recently introduced dependent typed higher-order logic (DHOL) offers an interesting compromise between expressiveness and automation support. It sacrifices the decidability of its type system in order to significantly extend its expressiveness over standard HOL. Yet it retains strong automated theorem proving support via a sound and complete translation to HOL. We leverage this design to extend DHOL with refinement and quotient types. Both of these are commonly requested by practitioners but rarely provided by automated theorem provers. This is because they inherently require undecidable typing and thus are very difficult to retrofit to decidable type systems. But with DHOL already doing the heavy lifting, adding them is not only possible but elegant and simple. Concretely, we add refinement and quotient types as special cases of subtyping. This turns the associated canonical inclusion resp. projection maps into identity maps and thus avoids costly changes in representation. We present the syntax, semantics, and translation to HOL for the extended language, including the proofs of soundness and completeness.",
      "url": "https://arxiv.org/abs/2507.02855v1",
      "source": "ArXiv Domain",
      "timestamp": "2025-07-03T17:59:00.000Z",
      "metadata": {
        "arxivId": "2507.02855v1",
        "authors": "Colin Rothgang, Florian Rabe",
        "categories": "cs.LO, cs.AI, cs.FL",
        "published": "2025-07-03T17:59:00Z",
        "pdfUrl": "https://arxiv.org/pdf/2507.02855v1.pdf",
        "abstractUrl": "https://arxiv.org/abs/2507.02855v1",
        "rank": 5,
        "domain": "AI"
      }
    },
    {
      "id": "arxiv-2408.05920v4-1751807425420",
      "title": "Urban Region Pre-training and Prompting: A Graph-based Approach",
      "description": "Urban region representation is crucial for various urban downstream tasks. However, despite the proliferation of methods and their success, acquiring general urban region knowledge and adapting to different tasks remains challenging. Existing work pays limited attention to the fine-grained functional layout semantics in urban regions, limiting their ability to capture transferable knowledge across regions. Further, inadequate handling of the unique features and relationships required for different downstream tasks may also hinder effective task adaptation. In this paper, we propose a $\\textbf{G}$raph-based $\\textbf{U}$rban $\\textbf{R}$egion $\\textbf{P}$re-training and $\\textbf{P}$rompting framework ($\\textbf{GURPP}$) for region representation learning. Specifically, we first construct an urban region graph and develop a subgraph-centric urban region pre-training model to capture the heterogeneous and transferable patterns of entity interactions. This model pre-trains knowledge-rich region embeddings using contrastive learning and multi-view learning methods. To further refine these representations, we design two graph-based prompting methods: a manually-defined prompt to incorporate explicit task knowledge and a task-learnable prompt to discover hidden knowledge, which enhances the adaptability of these embeddings to different tasks. Extensive experiments on various urban region prediction tasks and different cities demonstrate the superior performance of our framework.",
      "url": "https://arxiv.org/abs/2408.05920v4",
      "source": "ArXiv Domain",
      "timestamp": "2024-08-12T05:00:23.000Z",
      "metadata": {
        "arxivId": "2408.05920v4",
        "authors": "Jiahui Jin, Yifan Song, Dong Kan, Haojia Zhu, Xiangguo Sun, Zhicheng Li, Xigang Sun, Jinghui Zhang",
        "categories": "cs.AI, cs.LG",
        "published": "2024-08-12T05:00:23Z",
        "pdfUrl": "https://arxiv.org/pdf/2408.05920v4.pdf",
        "abstractUrl": "https://arxiv.org/abs/2408.05920v4",
        "rank": 6,
        "domain": "AI"
      }
    },
    {
      "id": "arxiv-2507.02851v1-1751807425420",
      "title": "MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs",
      "description": "Recent advancements in the reasoning capabilities of large language models (LLMs) show that employing group relative policy optimization (GRPO) algorithm for reinforcement learning (RL) training allows the models to use more thinking/reasoning tokens for generating better responses. However, LLMs can generate only a finite amount of tokens while maintaining attention to the previously generated tokens. This limit, also known as the context size of an LLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens. To think beyond the limit of context size, an LLM must employ a modular thinking strategy to reason over multiple rounds. In this work, we propose $\\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL training method for generating thinking tokens in multiple rounds, effectively allowing the model to think with additional context size. We trained the open-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient fine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our experiments show 3.8\\% and 3.3\\% improvements over vanilla GRPO based training in the respective benchmarks. Furthermore, this improvement was achieved with only 15\\% of samples, thus demonstrating sample efficiency of MOTIF. Our code and models are available at https://github.com/purbeshmitra/MOTIF and https://huggingface.co/purbeshmitra/MOTIF, respectively.",
      "url": "https://arxiv.org/abs/2507.02851v1",
      "source": "ArXiv Domain",
      "timestamp": "2025-07-03T17:55:43.000Z",
      "metadata": {
        "arxivId": "2507.02851v1",
        "authors": "Purbesh Mitra, Sennur Ulukus",
        "categories": "cs.CL, cs.AI, cs.IT, cs.LG, cs.SY, eess.SY, math.IT",
        "published": "2025-07-03T17:55:43Z",
        "pdfUrl": "https://arxiv.org/pdf/2507.02851v1.pdf",
        "abstractUrl": "https://arxiv.org/abs/2507.02851v1",
        "rank": 7,
        "domain": "AI"
      }
    },
    {
      "id": "arxiv-2507.02850v1-1751807425420",
      "title": "LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection to All Users",
      "description": "We describe a vulnerability in language models (LMs) trained with user feedback, whereby a single user can persistently alter LM knowledge and behavior given only the ability to provide prompts and upvote / downvote feedback on LM outputs. To implement the attack, the attacker prompts the LM to stochastically output either a \"poisoned\" or benign response, then upvotes the poisoned response or downvotes the benign one. When feedback signals are used in a subsequent preference tuning behavior, LMs exhibit increased probability of producing poisoned responses even in contexts without malicious prompts. We show that this attack can be used to (1) insert factual knowledge the model did not previously possess, (2) modify code generation patterns in ways that introduce exploitable security flaws, and (3) inject fake financial news. Our finding both identifies a new qualitative feature of language model preference tuning (showing that it even highly restricted forms of preference data can be used to exert fine-grained control over behavior), and a new attack mechanism for LMs trained with user feedback (extending work on pretraining-time data poisoning and deployment-time prompt injection).",
      "url": "https://arxiv.org/abs/2507.02850v1",
      "source": "ArXiv Domain",
      "timestamp": "2025-07-03T17:55:40.000Z",
      "metadata": {
        "arxivId": "2507.02850v1",
        "authors": "Almog Hilel, Idan Shenfeld, Leshem Choshen, Jacob Andreas",
        "categories": "cs.CL, cs.CR, cs.LG",
        "published": "2025-07-03T17:55:40Z",
        "pdfUrl": "https://arxiv.org/pdf/2507.02850v1.pdf",
        "abstractUrl": "https://arxiv.org/abs/2507.02850v1",
        "rank": 8,
        "domain": "AI"
      }
    },
    {
      "id": "arxiv-2507.02847v1-1751807425420",
      "title": "MvHo-IB: Multi-View Higher-Order Information Bottleneck for Brain Disorder Diagnosis",
      "description": "Recent evidence suggests that modeling higher-order interactions (HOIs) in functional magnetic resonance imaging (fMRI) data can enhance the diagnostic accuracy of machine learning systems. However, effectively extracting and utilizing HOIs remains a significant challenge. In this work, we propose MvHo-IB, a novel multi-view learning framework that integrates both pairwise interactions and HOIs for diagnostic decision-making, while automatically compressing task-irrelevant redundant information. MvHo-IB introduces several key innovations: (1) a principled method that combines O-information from information theory with a matrix-based Renyi alpha-order entropy estimator to quantify and extract HOIs, (2) a purpose-built Brain3DCNN encoder to effectively utilize these interactions, and (3) a new multi-view learning information bottleneck objective to enhance representation learning. Experiments on three benchmark fMRI datasets demonstrate that MvHo-IB achieves state-of-the-art performance, significantly outperforming previous methods, including recent hypergraph-based techniques. The implementation of MvHo-IB is available at https://github.com/zky04/MvHo-IB.",
      "url": "https://arxiv.org/abs/2507.02847v1",
      "source": "ArXiv Domain",
      "timestamp": "2025-07-03T17:54:03.000Z",
      "metadata": {
        "arxivId": "2507.02847v1",
        "authors": "Kunyu Zhang, Qiang Li, Shujian Yu",
        "categories": "cs.LG",
        "published": "2025-07-03T17:54:03Z",
        "pdfUrl": "https://arxiv.org/pdf/2507.02847v1.pdf",
        "abstractUrl": "https://arxiv.org/abs/2507.02847v1",
        "rank": 9,
        "domain": "AI"
      }
    },
    {
      "id": "arxiv-2507.02846v1-1751807425420",
      "title": "Legal Requirements Translation from Law",
      "description": "Software systems must comply with legal regulations, which is a resource-intensive task, particularly for small organizations and startups lacking dedicated legal expertise. Extracting metadata from regulations to elicit legal requirements for software is a critical step to ensure compliance. However, it is a cumbersome task due to the length and complex nature of legal text. Although prior work has pursued automated methods for extracting structural and semantic metadata from legal text, key limitations remain: they do not consider the interplay and interrelationships among attributes associated with these metadata types, and they rely on manual labeling or heuristic-driven machine learning, which does not generalize well to new documents. In this paper, we introduce an approach based on textual entailment and in-context learning for automatically generating a canonical representation of legal text, encodable and executable as Python code. Our representation is instantiated from a manually designed Python class structure that serves as a domain-specific metamodel, capturing both structural and semantic legal metadata and their interrelationships. This design choice reduces the need for large, manually labeled datasets and enhances applicability to unseen legislation. We evaluate our approach on 13 U.S. state data breach notification laws, demonstrating that our generated representations pass approximately 89.4% of test cases and achieve a precision and recall of 82.2 and 88.7, respectively.",
      "url": "https://arxiv.org/abs/2507.02846v1",
      "source": "ArXiv Domain",
      "timestamp": "2025-07-03T17:53:48.000Z",
      "metadata": {
        "arxivId": "2507.02846v1",
        "authors": "Anmol Singhal, Travis Breaux",
        "categories": "cs.SE, cs.CL",
        "published": "2025-07-03T17:53:48Z",
        "pdfUrl": "https://arxiv.org/pdf/2507.02846v1.pdf",
        "abstractUrl": "https://arxiv.org/abs/2507.02846v1",
        "rank": 10,
        "domain": "AI"
      }
    }
  ]
}